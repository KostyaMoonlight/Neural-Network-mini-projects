{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras vs TensorFlow vs PyTorch Cifar_10 comparison.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qKurpWXJdMsv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keras"
      ]
    },
    {
      "metadata": {
        "id": "vuwpUHKDKIKI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9FAoMzokOPwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=64\n",
        "EPOCHS = 25\n",
        "VAL_SPLIT = 0.85\n",
        "CONV_DROPOUT_RATE = 0.3\n",
        "FC_DROPOUT_RATE = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IzBtceWEK4ZT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "(y_train, y_test) = map(to_categorical, [y_train, y_test])\n",
        "split = int(len(x_train)*VAL_SPLIT)\n",
        "x_val = x_train[split:]\n",
        "y_val = y_train[split:]\n",
        "x_train = x_train[:split]\n",
        "y_train = y_train[:split]\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "  rescale=1./255,\n",
        "  rotation_range=20,\n",
        "  width_shift_range=0.2,\n",
        "  height_shift_range=0.2,\n",
        "  horizontal_flip=True)\n",
        "datagen.fit(x_train)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZS-ReV3nPdLA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_block(filters, filter_size=(3,3), pool_size=(2,2), pool_strides=(2,2), activation='relu', dropout_rate=CONV_DROPOUT_RATE):\n",
        "  def _conv_block(x):\n",
        "    x = Conv2D(filters, filter_size, activation=activation, padding='same')(x)    \n",
        "    x = Conv2D(filters, filter_size, activation=activation, padding='same')(x)\n",
        "    x = MaxPool2D(pool_size, pool_strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    return x\n",
        "  return _conv_block\n",
        "\n",
        "def dense_block(units, activation='relu', dropout_rate=FC_DROPOUT_RATE):\n",
        "  def _dense_block(x):\n",
        "    x = Dense(units, activation=activation)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    return x\n",
        "  return _dense_block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "knv6mgbpK6KI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "outputId": "3ba91b51-5a10-4491-a8c1-2a296911fda6"
      },
      "cell_type": "code",
      "source": [
        "input = Input([32,32,3])\n",
        "x = conv_block(32)(input) #[32,32,3] -> [16,16,16]\n",
        "x = conv_block(64)(x) #[16,16,16] -> [8,8,24]\n",
        "x = conv_block(128)(x) #[8,8,24] -> [4,4,32]\n",
        "x = Flatten()(x) #[4,4,128] -> [,512]\n",
        "x = dense_block(512, activation='relu')(x)\n",
        "out = Dense(10, activation='softmax')(x)\n",
        "model = Model(input, out)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,344,170\n",
            "Trainable params: 1,342,698\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v6bdV0_UK9F-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "outputId": "b3aeba12-b06f-4b9f-c9d0-d9bc6c286063"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "                    steps_per_epoch=len(x_train) / BATCH_SIZE, epochs=EPOCHS,\n",
        "                    validation_data=test_datagen.flow(x_val, y_val, batch_size=BATCH_SIZE),\n",
        "                    validation_steps=len(x_val) / BATCH_SIZE)\n",
        "result = model.evaluate_generator(test_datagen.flow(x_test, y_test, batch_size=BATCH_SIZE), steps=len(x_test) / BATCH_SIZE)\n",
        "print(result)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/25\n",
            "665/664 [==============================] - 25s 37ms/step - loss: 1.9996 - acc: 0.3463 - val_loss: 3.3672 - val_acc: 0.2304\n",
            "Epoch 2/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 1.4320 - acc: 0.4946 - val_loss: 1.4037 - val_acc: 0.5065\n",
            "Epoch 3/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 1.2448 - acc: 0.5571 - val_loss: 1.1242 - val_acc: 0.6101\n",
            "Epoch 4/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 1.1297 - acc: 0.5973 - val_loss: 1.9392 - val_acc: 0.4847\n",
            "Epoch 5/25\n",
            "665/664 [==============================] - 24s 36ms/step - loss: 1.0507 - acc: 0.6306 - val_loss: 1.9853 - val_acc: 0.4679\n",
            "Epoch 6/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.9941 - acc: 0.6550 - val_loss: 1.0380 - val_acc: 0.6523\n",
            "Epoch 7/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.9605 - acc: 0.6672 - val_loss: 0.9675 - val_acc: 0.6844\n",
            "Epoch 8/25\n",
            "665/664 [==============================] - 24s 36ms/step - loss: 0.9225 - acc: 0.6791 - val_loss: 1.9084 - val_acc: 0.4707\n",
            "Epoch 9/25\n",
            "665/664 [==============================] - 24s 35ms/step - loss: 0.8965 - acc: 0.6920 - val_loss: 1.5026 - val_acc: 0.5944\n",
            "Epoch 10/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.8658 - acc: 0.7006 - val_loss: 0.8290 - val_acc: 0.7288\n",
            "Epoch 11/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.8480 - acc: 0.7060 - val_loss: 1.0153 - val_acc: 0.6707\n",
            "Epoch 12/25\n",
            "665/664 [==============================] - 24s 36ms/step - loss: 0.8235 - acc: 0.7177 - val_loss: 0.9343 - val_acc: 0.7041\n",
            "Epoch 13/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.8091 - acc: 0.7241 - val_loss: 1.1077 - val_acc: 0.6700\n",
            "Epoch 14/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.7833 - acc: 0.7314 - val_loss: 0.7730 - val_acc: 0.7520\n",
            "Epoch 15/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.7677 - acc: 0.7339 - val_loss: 0.8353 - val_acc: 0.7360\n",
            "Epoch 16/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.7609 - acc: 0.7377 - val_loss: 0.6330 - val_acc: 0.7948\n",
            "Epoch 17/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.7512 - acc: 0.7410 - val_loss: 1.0367 - val_acc: 0.6829\n",
            "Epoch 18/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.7398 - acc: 0.7461 - val_loss: 0.7604 - val_acc: 0.7524\n",
            "Epoch 19/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.7225 - acc: 0.7501 - val_loss: 1.1667 - val_acc: 0.6799\n",
            "Epoch 20/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.7229 - acc: 0.7524 - val_loss: 1.0268 - val_acc: 0.7096\n",
            "Epoch 21/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.7093 - acc: 0.7569 - val_loss: 0.7412 - val_acc: 0.7577\n",
            "Epoch 22/25\n",
            "665/664 [==============================] - 25s 38ms/step - loss: 0.6929 - acc: 0.7642 - val_loss: 0.6886 - val_acc: 0.7760\n",
            "Epoch 23/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.6887 - acc: 0.7664 - val_loss: 0.9033 - val_acc: 0.7321\n",
            "Epoch 24/25\n",
            "665/664 [==============================] - 22s 34ms/step - loss: 0.6758 - acc: 0.7689 - val_loss: 0.6058 - val_acc: 0.8015\n",
            "Epoch 25/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.6717 - acc: 0.7713 - val_loss: 0.5752 - val_acc: 0.8017\n",
            "[0.6004480467796326, 0.7987]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mqY-TVdOdKkd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "bH2MBhlIK9tG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ndnpu5yHSaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32\n",
        "EPOCHS = 25\n",
        "VAL_SPLIT = 0.85\n",
        "CONV_DROPOUT_RATE = 0.3\n",
        "FC_DROPOUT_RATE = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jnQT7Jj-Voy5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "(y_train, y_test) = map(to_categorical, [y_train, y_test])\n",
        "x_train = x_train / 255.\n",
        "x_test = x_test / 255.\n",
        "split = int(len(x_train)*VAL_SPLIT)\n",
        "x_val = x_train[split:]\n",
        "y_val = y_train[split:]\n",
        "x_train = x_train[:split]\n",
        "y_train = y_train[:split]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "srsW93wjXsFZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def flip(x):\n",
        "  return tf.image.random_flip_left_right(x)\n",
        "     \n",
        "\n",
        "def color(x):\n",
        "  x = tf.image.random_hue(x, 0.08)\n",
        "  x = tf.image.random_saturation(x, 0.6, 1.6)\n",
        "  x = tf.image.random_brightness(x, 0.05)\n",
        "  x = tf.image.random_contrast(x, 0.7, 1.3)\n",
        "  return x\n",
        "\n",
        "def rotate():\n",
        "  random_angles = tf.random.uniform(shape = (tf.shape(x_train)[0], ), minval = -np.pi / 4, maxval = np.pi / 4)\n",
        "  return tf.contrib.image.angles_to_projective_transforms(random_angles, tf.cast(tf.shape(x_train)[1], tf.float32), tf.cast(tf.shape(x_train)[2], tf.float32))\n",
        "\n",
        "def zoom(x):\n",
        "  scales = list(np.arange(0.85, 1.0, 0.01))\n",
        "  boxes = np.zeros((len(scales), 4))\n",
        "\n",
        "  for i, scale in enumerate(scales):\n",
        "    x1 = y1 = 0.5 - (0.5 * scale)\n",
        "    x2 = y2 = 0.5 + (0.5 * scale)\n",
        "    boxes[i] = [x1, y1, x2, y2]\n",
        "\n",
        "  def random_crop(img):\n",
        "    crops = tf.image.crop_and_resize([img], boxes=boxes, box_ind=np.zeros(len(scales)), crop_size=(32, 32))\n",
        "    return crops[tf.random_uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
        "\n",
        "  choice = tf.random_uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "  return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x))\n",
        "\n",
        "augmentations = [flip, color] #zoom, rotate()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train.astype(np.int32)))\n",
        "dataset = dataset.shuffle(tf.cast(tf.shape(x_train)[0], tf.int64))\n",
        "\n",
        "for f in augmentations:\n",
        "    dataset = dataset.map(lambda x, y: tf.cond(tf.random_uniform([], 0, 1) > 0.75, lambda: (f(x),y), lambda: (x,y)), num_parallel_calls=4)\n",
        "dataset = dataset.map(lambda x,y: (tf.clip_by_value(x, 0, 1),y))\n",
        "dataset = dataset.batch(batch_size=BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=BATCH_SIZE)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((x_val, y_val.astype(np.int32))).batch(batch_size=BATCH_SIZE)\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test.astype(np.int32))).batch(batch_size=BATCH_SIZE)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9PojE3p4cx24",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Train():\n",
        "  \n",
        "  def __init__(self, datasets):\n",
        "    self.dataset, self.dataset_val, self.dataset_test = datasets\n",
        "    \n",
        "    self.build_model()\n",
        "  \n",
        "  def __conv_block(self, x, filters, filter_size=(3,3), pool_size=(2,2), pool_strides=(2,2), activation=tf.nn.relu):\n",
        "    x = tf.layers.conv2d(x, filters, kernel_size=filter_size, activation=activation, padding='same')\n",
        "    x = tf.layers.conv2d(x, filters, kernel_size=filter_size, activation=activation, padding='same')\n",
        "    x = tf.layers.max_pooling2d(x, pool_size, pool_strides, padding='same')\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "    x = tf.layers.dropout(x, self.conv_dropout_rate)\n",
        "    return x\n",
        "    \n",
        "  def __fc_block(self, x, units, activation=tf.nn.relu):\n",
        "    x = tf.layers.dense(x,units, activation=activation)\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "    x = tf.layers.dropout(x, self.fc_dropout_rate)\n",
        "    return x\n",
        "  \n",
        "  def build_model(self):\n",
        "    self.iter = iter = tf.data.Iterator.from_structure(self.dataset.output_types,self.dataset.output_shapes)\n",
        "    self.train_init_op = iter.make_initializer(self.dataset)    \n",
        "    self.val_init_op = iter.make_initializer(self.dataset_val)    \n",
        "    self.test_init_op = iter.make_initializer(self.dataset_test)\n",
        "\n",
        "    with tf.name_scope('placeholders'):\n",
        "      self.input_itr, self.labels_itr = self.iter.get_next()\n",
        "      self.batch_size = tf.placeholder(tf.int64)\n",
        "      self.input = tf.placeholder(tf.float32, [None, 32, 32, 3], name='input_ph')\n",
        "      self.labels = tf.placeholder(tf.int32, [None, 10], name='label_ph')\n",
        "      self.lr = tf.placeholder_with_default(0.001, [])    \n",
        "      self.conv_dropout_rate = tf.placeholder_with_default(1., [])\n",
        "      self.fc_dropout_rate = tf.placeholder_with_default(1., [])\n",
        "      \n",
        "    x = tf.cast(self.input_itr, tf.float32)\n",
        "    with tf.name_scope('conv_blocks'):\n",
        "      x = self.__conv_block(x, 32) #[32,32,3] -> [16,16,16]\n",
        "      x = self.__conv_block(x, 64) #[16,16,16] -> [8,8,24]\n",
        "      x = self.__conv_block(x, 128) #[8,8,24] -> [4,4,32]\n",
        "      \n",
        "    x = tf.layers.flatten(x) #[4,4,128] -> [,512]\n",
        "    \n",
        "    with tf.name_scope('fc_blocks'):\n",
        "      x = self.__fc_block(x, 512)\n",
        "\n",
        "    self.output = tf.layers.dense(x, 10, activation=tf.nn.softmax)\n",
        "    self.loss = tf.losses.softmax_cross_entropy(self.labels_itr, self.output)\n",
        "    self.accuracy = tf.metrics.accuracy(tf.argmax(self.labels_itr, axis=1), tf.argmax(self.output, axis=1))\n",
        "    self.train = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
        "    \n",
        "    \n",
        "  def eval(self,sess, init_op, x, y):\n",
        "    total_loss=[]\n",
        "    sess.run(init_op, feed_dict={ self.input: x, self.labels: y, self.batch_size: BATCH_SIZE})\n",
        "    for step in range(int(len(x)/BATCH_SIZE)):\n",
        "      _loss, _acc,  = sess.run([self.loss, self.accuracy])\n",
        "      total_loss.append(_loss)\n",
        "    loss = sum(total_loss)/len(total_loss)\n",
        "    return (loss, _acc)\n",
        "    \n",
        "      \n",
        "    \n",
        "  def train_model(self,epochs = 25, lr=0.01, decay=0.0):\n",
        "    #iter = self.dataset.make_initializable_iterator()\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      sess.run(tf.local_variables_initializer())\n",
        "      for epoch in range(epochs):\n",
        "        sess.run(self.train_init_op, feed_dict={ self.input: x_train, self.labels: y_train, self.batch_size: BATCH_SIZE})\n",
        "        total_train_loss=[]  \n",
        "        \n",
        "\n",
        "        for step in range(int(len(x_train)/BATCH_SIZE)):\n",
        "          _, _loss, _acc_train,  = sess.run([self.train, self.loss, self.accuracy], feed_dict={self.lr:lr, self.conv_dropout_rate:0.3, self.fc_dropout_rate:0.5})\n",
        "          total_train_loss.append(_loss)\n",
        "        \n",
        "        val_loss, val_acc = self.eval(sess,self.val_init_op, x_val, y_val)\n",
        "        print('epoch:',epoch+1,'  train loss:', sum(total_train_loss)/len(total_train_loss), ' train acc:',_acc_train ,'  val loss:', val_loss, ' val acc:',val_acc)\n",
        "      \n",
        "      test_loss, test_acc = self.eval(sess, self.test_init_op, x_test, y_test)\n",
        "      print('test loss:', test_loss, ' test acc:',test_acc )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BZtcJnruBart",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = Train((dataset, dataset_val, dataset_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pcB887YlBEYw",
        "colab_type": "code",
        "outputId": "71288849-f56e-4d20-d826-48f9ad8889ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "cell_type": "code",
      "source": [
        "train.train_model(epochs = EPOCHS, lr=0.0002)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1   train loss: 2.124059835291771  train acc: (0.3276187, 0.32763085)   val loss: 2.0531132144805713  val acc: (0.33836484, 0.3384083)\n",
            "epoch: 2   train loss: 1.999566840748471  train acc: (0.39302093, 0.39303634)   val loss: 1.951317752018953  val acc: (0.40191722, 0.4019286)\n",
            "epoch: 3   train loss: 1.923121237162366  train acc: (0.4420004, 0.44202045)   val loss: 1.8925133449399574  val acc: (0.44827908, 0.44828346)\n",
            "epoch: 4   train loss: 1.8708350303840924  train acc: (0.47948715, 0.47951135)   val loss: 1.848166875859611  val acc: (0.48449257, 0.48448503)\n",
            "epoch: 5   train loss: 1.82837546948927  train acc: (0.5103218, 0.5103493)   val loss: 1.828407769019787  val acc: (0.5140303, 0.51404047)\n",
            "epoch: 6   train loss: 1.7981736774904182  train acc: (0.53572357, 0.5357402)   val loss: 1.803622047106425  val acc: (0.5387499, 0.5387624)\n",
            "epoch: 7   train loss: 1.7706324977867574  train acc: (0.5575667, 0.5575818)   val loss: 1.7862015982978365  val acc: (0.56008756, 0.56009924)\n",
            "epoch: 8   train loss: 1.7493739765451615  train acc: (0.57657665, 0.5765882)   val loss: 1.7596073843475082  val acc: (0.57887906, 0.57888526)\n",
            "epoch: 9   train loss: 1.7303397528916957  train acc: (0.5936008, 0.59360534)   val loss: 1.758875413837596  val acc: (0.59540844, 0.5954083)\n",
            "epoch: 10   train loss: 1.7114804318870407  train acc: (0.608854, 0.60885507)   val loss: 1.750480753234309  val acc: (0.6103604, 0.6103593)\n",
            "epoch: 11   train loss: 1.6955853267965546  train acc: (0.6226618, 0.62267303)   val loss: 1.759445287223555  val acc: (0.6237086, 0.6237105)\n",
            "epoch: 12   train loss: 1.6804621624479812  train acc: (0.6350711, 0.6350773)   val loss: 1.7271881826922424  val acc: (0.6363359, 0.636337)\n",
            "epoch: 13   train loss: 1.669591164642788  train acc: (0.6466824, 0.6466891)   val loss: 1.7253295662056687  val acc: (0.6477161, 0.6477211)\n",
            "epoch: 14   train loss: 1.6553370392466167  train acc: (0.6575429, 0.6575472)   val loss: 1.7147522278321095  val acc: (0.6584894, 0.65849215)\n",
            "epoch: 15   train loss: 1.6431560526231686  train acc: (0.667749, 0.66775656)   val loss: 1.7063671763126667  val acc: (0.6686252, 0.66862595)\n",
            "epoch: 16   train loss: 1.634930911372943  train acc: (0.6771721, 0.67717886)   val loss: 1.705804269028525  val acc: (0.6778953, 0.6778944)\n",
            "epoch: 17   train loss: 1.6217954769191971  train acc: (0.6861132, 0.68611914)   val loss: 1.7109830501752021  val acc: (0.6866632, 0.68666446)\n",
            "epoch: 18   train loss: 1.6126024064170308  train acc: (0.6944413, 0.6944467)   val loss: 1.7012773604474516  val acc: (0.6949849, 0.69498575)\n",
            "epoch: 19   train loss: 1.6055739461837044  train acc: (0.70231146, 0.7023152)   val loss: 1.6952921656461863  val acc: (0.70281804, 0.70281756)\n",
            "epoch: 20   train loss: 1.5972153228449535  train acc: (0.7097625, 0.70976686)   val loss: 1.691641528891702  val acc: (0.710223, 0.71022224)\n",
            "epoch: 21   train loss: 1.5938212447855846  train acc: (0.7166582, 0.71666014)   val loss: 1.693070486570016  val acc: (0.7170122, 0.71701324)\n",
            "epoch: 22   train loss: 1.5856557940862266  train acc: (0.72322017, 0.7232237)   val loss: 1.691851747341645  val acc: (0.7235335, 0.7235352)\n",
            "epoch: 23   train loss: 1.5809643321547164  train acc: (0.7294162, 0.7294229)   val loss: 1.6866194030158541  val acc: (0.729706, 0.7297065)\n",
            "epoch: 24   train loss: 1.5751167813158897  train acc: (0.73533255, 0.735338)   val loss: 1.6835855087663374  val acc: (0.7355875, 0.7355887)\n",
            "epoch: 25   train loss: 1.5717973395822995  train acc: (0.74088305, 0.7408865)   val loss: 1.6884131141197987  val acc: (0.7410689, 0.74106914)\n",
            "test loss: 1.6871983168216853  test acc: (0.74132705, 0.7413249)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AaSjYqlR8WWL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "0cGCLQHBdHGD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0dBYzxqYL5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32\n",
        "EPOCHS = 25\n",
        "VAL_SPLIT = 0.85\n",
        "SHUFFLE = True\n",
        "SHUFFLE_SEED = 19\n",
        "CONV_DROPOUT_RATE = 0.3\n",
        "FC_DROPOUT_RATE = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdVabRNfdah6",
        "colab_type": "code",
        "outputId": "dbb27c54-13b3-4699-b5bb-5c56660e7705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose(\n",
        "    [transforms.ColorJitter(brightness=0.05, contrast=(0.7,1.3), saturation=(0.6,1.6), hue=(-0.2,0.2)),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_test)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(VAL_SPLIT * num_train))\n",
        "\n",
        "if SHUFFLE:\n",
        "  np.random.seed(SHUFFLE_SEED)\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[:split], indices[split:]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=False, sampler=train_sampler, num_workers=4)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=False, sampler=valid_sampler, num_workers=4)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gFdtYLysdion",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "f4eddb5c-3fa8-4376-d5dd-cf3069be5050"
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(3, 32, 3, padding=1)        \n",
        "        self.conv1_2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.dropout1 = nn.Dropout(CONV_DROPOUT_RATE)\n",
        "        \n",
        "        self.conv2_1 = nn.Conv2d(32, 64, 3, padding=1)        \n",
        "        self.conv2_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.dropout2 = nn.Dropout(CONV_DROPOUT_RATE)\n",
        "        \n",
        "        self.conv3_1 = nn.Conv2d(64, 128, 3, padding=1)        \n",
        "        self.conv3_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.dropout3 = nn.Dropout(CONV_DROPOUT_RATE)\n",
        "        \n",
        "        self.fc4 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.dropout4 = nn.Dropout(FC_DROPOUT_RATE)\n",
        "        self.output = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout3(x)\n",
        " \n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout4(x)\n",
        "        \n",
        "        x = F.softmax(self.output(x), 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "net.to(device)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1_1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv1_2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout1): Dropout(p=0.3)\n",
              "  (conv2_1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout2): Dropout(p=0.3)\n",
              "  (conv3_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout3): Dropout(p=0.3)\n",
              "  (fc4): Linear(in_features=2048, out_features=512, bias=True)\n",
              "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (dropout4): Dropout(p=0.5)\n",
              "  (output): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "BD8tvjmwdqkC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0002)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iEmIASeJTx50",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval(loader):\n",
        "  total_loss = 0.0\n",
        "  total_acc = 0.0\n",
        "  sample_len = 0\n",
        "  for i, data in enumerate(valloader, 0):\n",
        "\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    \n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    _, prediction = torch.max(outputs.data, 1)\n",
        "    sample_len += len(prediction)\n",
        "    total_acc += torch.sum(prediction == labels.data)\n",
        "  loss = total_loss/i \n",
        "  acc = total_acc.item()/ sample_len\n",
        "  return (loss, acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zufkaddie2Io",
        "colab_type": "code",
        "outputId": "fb410a4d-ece4-4fbc-e784-f24f165a5309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "    train_loss = 0.0   \n",
        "    train_acc = 0.0\n",
        "    train_sample_len = 0\n",
        "    \n",
        "    #Train steps\n",
        "    net.train()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      _, prediction = torch.max(outputs.data, 1)\n",
        "      train_sample_len += len(prediction)\n",
        "      train_acc += torch.sum(prediction == labels.data)\n",
        "    train_loss = train_loss/i\n",
        "    train_acc = train_acc.item()/ train_sample_len\n",
        "    \n",
        "    #Validation steps\n",
        "    net.eval()\n",
        "    val_loss, val_acc = eval(valloader)\n",
        "    print('epoch is :', epoch+1, '\\tloss:',train_loss,  '\\tacc:',train_acc)\n",
        "    print('\\t\\tloss:',val_loss,  '\\tacc:',val_acc)\n",
        "\n",
        "#Test steps  \n",
        "test_loss, test_acc = eval(testloader)\n",
        "print('Test results:\\tloss:',test_loss,  '\\tacc:',test_acc)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is : 1 \tloss: 1.8965675599244705 \tacc: 0.5751058823529411\n",
            "\t\tloss: 1.830004265675178 \tacc: 0.6429333333333334\n",
            "epoch is : 2 \tloss: 1.8353421528296299 \tacc: 0.6346117647058823\n",
            "\t\tloss: 1.7814894909532661 \tacc: 0.6894666666666667\n",
            "epoch is : 3 \tloss: 1.8015582685370044 \tacc: 0.6666588235294117\n",
            "\t\tloss: 1.7493902368423266 \tacc: 0.7241333333333333\n",
            "epoch is : 4 \tloss: 1.7832308264381915 \tacc: 0.6840705882352941\n",
            "\t\tloss: 1.7325530678797991 \tacc: 0.7376\n",
            "epoch is : 5 \tloss: 1.7642069221081504 \tacc: 0.7023764705882353\n",
            "\t\tloss: 1.7277796604694464 \tacc: 0.7417333333333334\n",
            "epoch is : 6 \tloss: 1.7536409687565033 \tacc: 0.7115058823529412\n",
            "\t\tloss: 1.7155056279948635 \tacc: 0.754\n",
            "epoch is : 7 \tloss: 1.7417139505226928 \tacc: 0.7224705882352941\n",
            "\t\tloss: 1.7090580611147432 \tacc: 0.7598666666666667\n",
            "epoch is : 8 \tloss: 1.7321280267942383 \tacc: 0.7323764705882353\n",
            "\t\tloss: 1.6927537062229254 \tacc: 0.7778666666666667\n",
            "epoch is : 9 \tloss: 1.7234426101288163 \tacc: 0.7422352941176471\n",
            "\t\tloss: 1.6853676886640043 \tacc: 0.7841333333333333\n",
            "epoch is : 10 \tloss: 1.719451562557594 \tacc: 0.7451529411764706\n",
            "\t\tloss: 1.6940880993492582 \tacc: 0.7750666666666667\n",
            "epoch is : 11 \tloss: 1.7140339758381786 \tacc: 0.7506823529411765\n",
            "\t\tloss: 1.675765079820258 \tacc: 0.7938666666666667\n",
            "epoch is : 12 \tloss: 1.7096748913088478 \tacc: 0.7543764705882353\n",
            "\t\tloss: 1.6715536560767736 \tacc: 0.7968\n",
            "epoch is : 13 \tloss: 1.7035293064742203 \tacc: 0.7605647058823529\n",
            "\t\tloss: 1.6686787921139317 \tacc: 0.8012\n",
            "epoch is : 14 \tloss: 1.6966763301247574 \tacc: 0.7675294117647059\n",
            "\t\tloss: 1.666256267290849 \tacc: 0.8028\n",
            "epoch is : 15 \tloss: 1.695325892553272 \tacc: 0.7684235294117647\n",
            "\t\tloss: 1.6648799481554928 \tacc: 0.8038666666666666\n",
            "epoch is : 16 \tloss: 1.6895942393555698 \tacc: 0.7742117647058824\n",
            "\t\tloss: 1.6606215075549917 \tacc: 0.8065333333333333\n",
            "epoch is : 17 \tloss: 1.6859993236251625 \tacc: 0.7769411764705882\n",
            "\t\tloss: 1.6571301435812926 \tacc: 0.8108\n",
            "epoch is : 18 \tloss: 1.68423795592354 \tacc: 0.7792705882352942\n",
            "\t\tloss: 1.6508176255429912 \tacc: 0.8162666666666667\n",
            "epoch is : 19 \tloss: 1.679713665182332 \tacc: 0.7840470588235294\n",
            "\t\tloss: 1.653359333674113 \tacc: 0.8146666666666667\n",
            "epoch is : 20 \tloss: 1.6751935339297157 \tacc: 0.7883764705882353\n",
            "\t\tloss: 1.650661474109715 \tacc: 0.8168\n",
            "epoch is : 21 \tloss: 1.6738795069865433 \tacc: 0.7899058823529411\n",
            "\t\tloss: 1.6515693659456367 \tacc: 0.8190666666666667\n",
            "epoch is : 22 \tloss: 1.6719677657427559 \tacc: 0.7911294117647059\n",
            "\t\tloss: 1.6465544293069432 \tacc: 0.8217333333333333\n",
            "epoch is : 23 \tloss: 1.6682386393827129 \tacc: 0.7957882352941177\n",
            "\t\tloss: 1.6477062406702938 \tacc: 0.8209333333333333\n",
            "epoch is : 24 \tloss: 1.6660725609904312 \tacc: 0.7972235294117647\n",
            "\t\tloss: 1.6434759507831345 \tacc: 0.8234666666666667\n",
            "epoch is : 25 \tloss: 1.6653474721922932 \tacc: 0.798235294117647\n",
            "\t\tloss: 1.6388876056059813 \tacc: 0.8297333333333333\n",
            "Test results:\tloss: 1.6392629447146359 \tacc: 0.8297333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}