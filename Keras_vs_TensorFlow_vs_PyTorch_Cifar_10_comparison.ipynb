{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras vs TensorFlow vs PyTorch Cifar_10 comparison.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "qKurpWXJdMsv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keras"
      ]
    },
    {
      "metadata": {
        "id": "vuwpUHKDKIKI",
        "colab_type": "code",
        "outputId": "e8dcdd32-a0ef-4892-bc45-0d3db47e5349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9FAoMzokOPwc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=64\n",
        "EPOCHS = 25\n",
        "VAL_SPLIT = 0.85\n",
        "CONV_DROPOUT_RATE = 0.3\n",
        "FC_DROPOUT_RATE = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IzBtceWEK4ZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4147a50d-e3c8-44c4-f232-f1d8540220e9"
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "(y_train, y_test) = map(to_categorical, [y_train, y_test])\n",
        "split = int(len(x_train)*VAL_SPLIT)\n",
        "x_val = x_train[split:]\n",
        "y_val = y_train[split:]\n",
        "x_train = x_train[:split]\n",
        "y_train = y_train[:split]\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "  rescale=1./255,\n",
        "  rotation_range=20,\n",
        "  width_shift_range=0.2,\n",
        "  height_shift_range=0.2,\n",
        "  horizontal_flip=True)\n",
        "datagen.fit(x_train)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 29s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZS-ReV3nPdLA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv_block(filters, filter_size=(3,3), pool_size=(2,2), pool_strides=(2,2), activation='relu', dropout_rate=CONV_DROPOUT_RATE):\n",
        "  def _conv_block(x):\n",
        "    x = Conv2D(filters, filter_size, activation=activation, padding='same')(x)    \n",
        "    x = Conv2D(filters, filter_size, activation=activation, padding='same')(x)\n",
        "    x = MaxPool2D(pool_size, pool_strides, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    return x\n",
        "  return _conv_block\n",
        "\n",
        "def dense_block(units, activation='relu', dropout_rate=FC_DROPOUT_RATE):\n",
        "  def _dense_block(x):\n",
        "    x = Dense(units, activation=activation)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    return x\n",
        "  return _dense_block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "knv6mgbpK6KI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "1a587fba-6c1d-46f5-f5d4-26442ded7cba"
      },
      "cell_type": "code",
      "source": [
        "input = Input([32,32,3])\n",
        "x = conv_block(32)(input) #[32,32,3] -> [16,16,16]\n",
        "x = conv_block(64)(x) #[16,16,16] -> [8,8,24]\n",
        "x = conv_block(128)(x) #[8,8,24] -> [4,4,32]\n",
        "x = Flatten()(x) #[4,4,128] -> [,512]\n",
        "x = dense_block(512, activation='relu')(x)\n",
        "out = Dense(10, activation='softmax')(x)\n",
        "model = Model(input, out)\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,344,170\n",
            "Trainable params: 1,342,698\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v6bdV0_UK9F-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "outputId": "eece16a5-4f46-44eb-86a2-aa41bb520c1f"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "                    steps_per_epoch=len(x_train) / BATCH_SIZE, epochs=EPOCHS,\n",
        "                    validation_data=test_datagen.flow(x_val, y_val, batch_size=BATCH_SIZE),\n",
        "                    validation_steps=len(x_val) / BATCH_SIZE)\n",
        "result = model.evaluate_generator(test_datagen.flow(x_test, y_test, batch_size=BATCH_SIZE), steps=len(x_test) / BATCH_SIZE)\n",
        "print(result)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/25\n",
            "665/664 [==============================] - 24s 36ms/step - loss: 2.0134 - acc: 0.3431 - val_loss: 3.5915 - val_acc: 0.2443\n",
            "Epoch 2/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 1.4274 - acc: 0.4972 - val_loss: 1.5198 - val_acc: 0.4892\n",
            "Epoch 3/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 1.2282 - acc: 0.5665 - val_loss: 1.3324 - val_acc: 0.5475\n",
            "Epoch 4/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 1.1224 - acc: 0.6021 - val_loss: 2.0549 - val_acc: 0.4117\n",
            "Epoch 5/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 1.0489 - acc: 0.6337 - val_loss: 1.6919 - val_acc: 0.5864\n",
            "Epoch 6/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.9924 - acc: 0.6526 - val_loss: 1.1169 - val_acc: 0.6440\n",
            "Epoch 7/25\n",
            "665/664 [==============================] - 23s 34ms/step - loss: 0.9567 - acc: 0.6654 - val_loss: 1.4936 - val_acc: 0.5852\n",
            "Epoch 8/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.9233 - acc: 0.6803 - val_loss: 1.1442 - val_acc: 0.6613\n",
            "Epoch 9/25\n",
            "665/664 [==============================] - 24s 35ms/step - loss: 0.8994 - acc: 0.6887 - val_loss: 0.7898 - val_acc: 0.7341\n",
            "Epoch 10/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.8717 - acc: 0.7012 - val_loss: 1.1635 - val_acc: 0.6532\n",
            "Epoch 11/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.8475 - acc: 0.7065 - val_loss: 0.9929 - val_acc: 0.7079\n",
            "Epoch 12/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.8300 - acc: 0.7149 - val_loss: 0.7718 - val_acc: 0.7409\n",
            "Epoch 13/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.8117 - acc: 0.7222 - val_loss: 1.2036 - val_acc: 0.6244\n",
            "Epoch 14/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.7868 - acc: 0.7291 - val_loss: 0.7869 - val_acc: 0.7441\n",
            "Epoch 15/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.7808 - acc: 0.7329 - val_loss: 0.8373 - val_acc: 0.7213\n",
            "Epoch 16/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.7650 - acc: 0.7389 - val_loss: 1.0243 - val_acc: 0.6691\n",
            "Epoch 17/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.7556 - acc: 0.7420 - val_loss: 0.7395 - val_acc: 0.7528\n",
            "Epoch 18/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.7445 - acc: 0.7462 - val_loss: 1.0102 - val_acc: 0.7057\n",
            "Epoch 19/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.7328 - acc: 0.7509 - val_loss: 0.7394 - val_acc: 0.7581\n",
            "Epoch 20/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.7178 - acc: 0.7547 - val_loss: 0.6256 - val_acc: 0.7951\n",
            "Epoch 21/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.7148 - acc: 0.7573 - val_loss: 0.9158 - val_acc: 0.7197\n",
            "Epoch 22/25\n",
            "665/664 [==============================] - 25s 37ms/step - loss: 0.7014 - acc: 0.7597 - val_loss: 0.8772 - val_acc: 0.7304\n",
            "Epoch 23/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.7055 - acc: 0.7602 - val_loss: 0.6757 - val_acc: 0.7707\n",
            "Epoch 24/25\n",
            "665/664 [==============================] - 22s 33ms/step - loss: 0.6908 - acc: 0.7640 - val_loss: 0.7097 - val_acc: 0.7573\n",
            "Epoch 25/25\n",
            "665/664 [==============================] - 23s 35ms/step - loss: 0.6796 - acc: 0.7651 - val_loss: 0.7453 - val_acc: 0.7591\n",
            "[0.7755924012184143, 0.7526]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mqY-TVdOdKkd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "bH2MBhlIK9tG",
        "colab_type": "code",
        "outputId": "7d3596e1-4cdc-4aa5-8408-d943274d58f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "7ndnpu5yHSaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32\n",
        "EPOCHS = 25\n",
        "VAL_SPLIT = 0.85\n",
        "CONV_DROPOUT_RATE = 0.3\n",
        "FC_DROPOUT_RATE = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jnQT7Jj-Voy5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "296013b8-a81e-4a5a-afb2-722535676a01"
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "(y_train, y_test) = map(to_categorical, [y_train, y_test])\n",
        "x_train = x_train / 255.\n",
        "x_test = x_test / 255.\n",
        "split = int(len(x_train)*VAL_SPLIT)\n",
        "x_val = x_train[split:]\n",
        "y_val = y_train[split:]\n",
        "x_train = x_train[:split]\n",
        "y_train = y_train[:split]\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 56s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "srsW93wjXsFZ",
        "colab_type": "code",
        "outputId": "e08f4eaf-b4df-4f70-c72e-c82782625cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "def flip(x):\n",
        "  return tf.image.random_flip_left_right(x)\n",
        "     \n",
        "\n",
        "def color(x):\n",
        "  x = tf.image.random_hue(x, 0.08)\n",
        "  x = tf.image.random_saturation(x, 0.6, 1.6)\n",
        "  x = tf.image.random_brightness(x, 0.05)\n",
        "  x = tf.image.random_contrast(x, 0.7, 1.3)\n",
        "  return x\n",
        "\n",
        "def rotate():\n",
        "  random_angles = tf.random.uniform(shape = (tf.shape(x_train)[0], ), minval = -np.pi / 4, maxval = np.pi / 4)\n",
        "  return tf.contrib.image.angles_to_projective_transforms(random_angles, tf.cast(tf.shape(x_train)[1], tf.float32), tf.cast(tf.shape(x_train)[2], tf.float32))\n",
        "\n",
        "def zoom(x):\n",
        "  scales = list(np.arange(0.85, 1.0, 0.01))\n",
        "  boxes = np.zeros((len(scales), 4))\n",
        "\n",
        "  for i, scale in enumerate(scales):\n",
        "    x1 = y1 = 0.5 - (0.5 * scale)\n",
        "    x2 = y2 = 0.5 + (0.5 * scale)\n",
        "    boxes[i] = [x1, y1, x2, y2]\n",
        "\n",
        "  def random_crop(img):\n",
        "    crops = tf.image.crop_and_resize([img], boxes=boxes, box_ind=np.zeros(len(scales)), crop_size=(32, 32))\n",
        "    return crops[tf.random_uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]\n",
        "\n",
        "  choice = tf.random_uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "  return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x))\n",
        "\n",
        "augmentations = [flip, color] #zoom, rotate()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train.astype(np.int32)))\n",
        "dataset = dataset.shuffle(tf.cast(tf.shape(x_train)[0], tf.int64))\n",
        "\n",
        "for f in augmentations:\n",
        "    dataset = dataset.map(lambda x, y: tf.cond(tf.random_uniform([], 0, 1) > 0.75, lambda: (f(x),y), lambda: (x,y)), num_parallel_calls=4)\n",
        "dataset = dataset.map(lambda x,y: (tf.clip_by_value(x, 0, 1),y))\n",
        "dataset = dataset.batch(batch_size=BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=BATCH_SIZE)\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((x_val, y_val.astype(np.int32))).batch(batch_size=BATCH_SIZE)\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test.astype(np.int32))).batch(batch_size=BATCH_SIZE)\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9PojE3p4cx24",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Train():\n",
        "  \n",
        "  def __init__(self, datasets):\n",
        "    self.dataset, self.dataset_val, self.dataset_test = datasets\n",
        "    \n",
        "    self.build_model()\n",
        "  \n",
        "  def __conv_block(self, x, filters, filter_size=(3,3), pool_size=(2,2), pool_strides=(2,2), activation=tf.nn.relu):\n",
        "    x = tf.layers.conv2d(x, filters, kernel_size=filter_size, activation=activation, padding='same')\n",
        "    x = tf.layers.conv2d(x, filters, kernel_size=filter_size, activation=activation, padding='same')\n",
        "    x = tf.layers.max_pooling2d(x, pool_size, pool_strides, padding='same')\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "    x = tf.layers.dropout(x, self.conv_dropout_rate)\n",
        "    return x\n",
        "    \n",
        "  def __fc_block(self, x, units, activation=tf.nn.relu):\n",
        "    x = tf.layers.dense(x,units, activation=activation)\n",
        "    x = tf.layers.batch_normalization(x)\n",
        "    x = tf.layers.dropout(x, self.fc_dropout_rate)\n",
        "    return x\n",
        "  \n",
        "  def build_model(self):\n",
        "    self.iter = iter = tf.data.Iterator.from_structure(self.dataset.output_types,self.dataset.output_shapes)\n",
        "    self.train_init_op = iter.make_initializer(self.dataset)    \n",
        "    self.val_init_op = iter.make_initializer(self.dataset_val)    \n",
        "    self.test_init_op = iter.make_initializer(self.dataset_test)\n",
        "\n",
        "    with tf.name_scope('placeholders'):\n",
        "      self.input_itr, self.labels_itr = self.iter.get_next()\n",
        "      self.batch_size = tf.placeholder(tf.int64)\n",
        "      self.input = tf.placeholder(tf.float32, [None, 32, 32, 3], name='input_ph')\n",
        "      self.labels = tf.placeholder(tf.int32, [None, 10], name='label_ph')\n",
        "      self.lr = tf.placeholder_with_default(0.001, [])    \n",
        "      self.conv_dropout_rate = tf.placeholder_with_default(1., [])\n",
        "      self.fc_dropout_rate = tf.placeholder_with_default(1., [])\n",
        "      \n",
        "    x = tf.cast(self.input_itr, tf.float32)\n",
        "    with tf.name_scope('conv_blocks'):\n",
        "      x = self.__conv_block(x, 32) #[32,32,3] -> [16,16,16]\n",
        "      x = self.__conv_block(x, 64) #[16,16,16] -> [8,8,24]\n",
        "      x = self.__conv_block(x, 128) #[8,8,24] -> [4,4,32]\n",
        "      \n",
        "    x = tf.layers.flatten(x) #[4,4,128] -> [,512]\n",
        "    \n",
        "    with tf.name_scope('fc_blocks'):\n",
        "      x = self.__fc_block(x, 512)\n",
        "\n",
        "    self.output = tf.layers.dense(x, 10, activation=tf.nn.softmax)\n",
        "    self.loss = tf.losses.softmax_cross_entropy(self.labels_itr, self.output)\n",
        "    self.accuracy = tf.metrics.accuracy(tf.argmax(self.labels_itr, axis=1), tf.argmax(self.output, axis=1))\n",
        "    self.train = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
        "    \n",
        "    \n",
        "  def eval(self,sess, init_op, x, y):\n",
        "    total_loss=[]\n",
        "    sess.run(init_op, feed_dict={ self.input: x, self.labels: y, self.batch_size: BATCH_SIZE})\n",
        "    for step in range(int(len(x)/BATCH_SIZE)):\n",
        "      _loss, _acc,  = sess.run([self.loss, self.accuracy])\n",
        "      total_loss.append(_loss)\n",
        "    loss = sum(total_loss)/len(total_loss)\n",
        "    return (loss, _acc)\n",
        "    \n",
        "      \n",
        "    \n",
        "  def train_model(self,epochs = 25, lr=0.01, decay=0.0):\n",
        "    #iter = self.dataset.make_initializable_iterator()\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      sess.run(tf.local_variables_initializer())\n",
        "      for epoch in range(epochs):\n",
        "        sess.run(self.train_init_op, feed_dict={ self.input: x_train, self.labels: y_train, self.batch_size: BATCH_SIZE})\n",
        "        total_train_loss=[]  \n",
        "        \n",
        "\n",
        "        for step in range(int(len(x_train)/BATCH_SIZE)):\n",
        "          _, _loss, _acc_train,  = sess.run([self.train, self.loss, self.accuracy], feed_dict={self.lr:lr, self.conv_dropout_rate:0.3, self.fc_dropout_rate:0.5})\n",
        "          total_train_loss.append(_loss)\n",
        "        \n",
        "        val_loss, val_acc = self.eval(self.val_init_op, x_val, y_val)\n",
        "        #sess.run(self.val_init_op, feed_dict={ self.input: x_val, self.labels: y_val, self.batch_size: BATCH_SIZE})\n",
        "        #for step in range(int(len(x_val)/BATCH_SIZE)):\n",
        "        #  _loss, _acc_val,  = sess.run([self.loss, self.accuracy])\n",
        "        #  total_val_loss.append(_loss)\n",
        "        print('epoch:',epoch+1,'  train loss:', sum(total_train_loss)/len(total_train_loss), ' train acc:',_acc_train ,'  val loss:', val_loss, ' val acc:',val_acc)\n",
        "      \n",
        "      test_loss, test_acc = self.eval(self.test_init_op, x_test, y_test)\n",
        "      #sess.run(self.test_init_op, feed_dict={ self.input: x_test, self.labels: y_test, self.batch_size: BATCH_SIZE})\n",
        "      #total_test_loss=[]  \n",
        "      #for step in range(int(len(x_test)/BATCH_SIZE)):\n",
        "      #  _loss, _acc_test,  = sess.run([self.loss, self.accuracy])\n",
        "      #  total_test_loss.append(_loss)\n",
        "      print('test loss:', test_loss, ' test acc:',test_acc )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BZtcJnruBart",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "9966ec54-c612-4742-cc82-e60ffda959c7"
      },
      "cell_type": "code",
      "source": [
        "train = Train((dataset, dataset_val, dataset_test))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-ab8722342885>:9: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-ab8722342885>:11: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-ab8722342885>:12: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-ab8722342885>:13: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-ab8722342885>:43: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-ab8722342885>:17: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pcB887YlBEYw",
        "colab_type": "code",
        "outputId": "94838a35-eca0-4c23-ff34-19b624305dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "train.train_model(epochs = EPOCHS, lr=0.0002)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-923329632a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-ab8722342885>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, epochs, lr, decay)\u001b[0m\n\u001b[1;32m     77\u001b[0m           \u001b[0mtotal_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_init_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;31m#sess.run(self.val_init_op, feed_dict={ self.input: x_val, self.labels: y_val, self.batch_size: BATCH_SIZE})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m#for step in range(int(len(x_val)/BATCH_SIZE)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: eval() missing 1 required positional argument: 'y'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "AaSjYqlR8WWL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "0cGCLQHBdHGD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0dBYzxqYL5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32\n",
        "EPOCHS = 25\n",
        "VAL_SPLIT = 0.85\n",
        "SHUFFLE = True\n",
        "SHUFFLE_SEED = 19\n",
        "CONV_DROPOUT_RATE = 0.3\n",
        "FC_DROPOUT_RATE = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdVabRNfdah6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose(\n",
        "    [transforms.ColorJitter(brightness=0.05, contrast=(0.7,1.3), saturation=(0.6,1.6), hue=(-0.2,0.2)),\n",
        "     transforms.RandomHorizontalFlip(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_test)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "\n",
        "num_train = len(trainset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(VAL_SPLIT * num_train))\n",
        "#trainset, valset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "if SHUFFLE:\n",
        "  np.random.seed(SHUFFLE_SEED)\n",
        "  np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[:split], indices[split:]\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, sampler=trainset, num_workers=4)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, sampler=valid_sampler, num_workers=4)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFdtYLysdion",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1_1 = nn.Conv2d(3, 32, 3, padding=1)        \n",
        "        self.conv1_2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.dropout1 = nn.Dropout(CONV_DROPOUT_RATE)\n",
        "        \n",
        "        self.conv2_1 = nn.Conv2d(32, 64, 3, padding=1)        \n",
        "        self.conv2_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.dropout2 = nn.Dropout(CONV_DROPOUT_RATE)\n",
        "        \n",
        "        self.conv3_1 = nn.Conv2d(64, 128, 3, padding=1)        \n",
        "        self.conv3_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.dropout3 = nn.Dropout(CONV_DROPOUT_RATE)\n",
        "        \n",
        "        self.fc4 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.dropout4 = nn.Dropout(FC_DROPOUT_RATE)\n",
        "        self.output = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1_1(x))\n",
        "        x = F.relu(self.conv1_2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = F.relu(self.conv2_1(x))\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout2(x)\n",
        "        \n",
        "        x = F.relu(self.conv3_1(x))\n",
        "        x = F.relu(self.conv3_2(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout3(x)\n",
        " \n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout4(x)\n",
        "        \n",
        "        x = F.softmax(self.output(x), 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "net.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BD8tvjmwdqkC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0002)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iEmIASeJTx50",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval(loader):\n",
        "  total_loss = 0.0\n",
        "  total_acc = 0.0\n",
        "  sample_len = 0\n",
        "  for i, data in enumerate(valloader, 0):\n",
        "\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    \n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    _, prediction = torch.max(outputs.data, 1)\n",
        "    sample_len += len(prediction)\n",
        "    total_acc += torch.sum(prediction == labels.data)\n",
        "  loss = val_loss/i \n",
        "  acc = total_acc.item()/ sample_len\n",
        "  return (loss, acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zufkaddie2Io",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
        "    train_loss = 0.0   \n",
        "    train_acc = 0.0\n",
        "    train_sample_len = 0\n",
        "    \n",
        "    #Train steps\n",
        "    net.train()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = net(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss += loss.item()\n",
        "      _, prediction = torch.max(outputs.data, 1)\n",
        "      train_sample_len += len(prediction)\n",
        "      train_acc += torch.sum(prediction == labels.data)\n",
        "    train_loss = train_loss/i\n",
        "    train_acc = train_acc.item()/ train_sample_len\n",
        "    \n",
        "    #Validation steps\n",
        "    net.eval()\n",
        "    val_loss, val_acc = eval(valloader)\n",
        "    print('epoch is :', epoch+1, '\\tloss:',train_loss,  '\\tacc:',train_acc)\n",
        "    print('\\t\\tloss:',val_loss,  '\\tacc:',val_acc)\n",
        "\n",
        "#Test steps  \n",
        "test_loss, test_acc = eval(testloader)\n",
        "print('Test results:\\tloss:',test_loss,  '\\tacc:',test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}